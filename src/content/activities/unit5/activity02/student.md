El proyecto es una experiencia audiovisual en vivo donde la música transforma visuales en tiempo real de una forma más humana. En lugar de patrones abstractos como líneas o partículas, las proyecciones reaccionarán a la intensidad y emoción del sonido, modificando rostros, cuerpos y escenarios.

El evento será un concierto o presentación en vivo donde el público no solo escuche la música, sino que la vea reflejada de manera tangible. La narrativa se basa en la sinestesia, la capacidad de "ver" la música, y evoluciona según la interacción del público y los cambios en la música.

Para lograrlo, se usarán cuatro dispositivos conectados: uno principal para la proyección, dos que capturarán datos del público (movimiento, expresiones o sonido ambiente) y uno de control remoto para ajustar variables en tiempo real. Se utilizarán tecnologías web como P5.js o Three.js para la generación de visuales, con WebSockets para la interacción en vivo.

El objetivo es explorar cómo el arte generativo puede ser más expresivo y cercano a la audiencia, creando una experiencia inmersiva donde la música y la imagen se fusionen de manera emocional y dinámica.


Ejemplo
![hq720](https://github.com/user-attachments/assets/284f031d-81d8-47f0-91ad-3e39da214a50)

en el after life se colocan estas visuales, lo que quiero hacer es poder intervenir en estas y alteralas en vivo, por medio de diferentes dispositivos que hayan conectados a la red del evento

quedando algo asi 

![hq720](https://github.com/user-attachments/assets/28e40d63-3e2e-4f7c-83da-720383aeb8d6)

Dentro de todo quiero que haya una conexion tal que asi

![image](https://github.com/user-attachments/assets/acb937d0-a846-42da-9aed-7b769b938a25)
